{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Control Project\n",
    "This is the report on my solution of the Continuous Control Project from the [Udacity deep reinforcement learning course](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation notes\n",
    "I decided to use the Single Reacher Environment provided by Udacity. I then used the [Facade design pattern](https://en.wikipedia.org/wiki/Facade_pattern) to create a facade between Unity ML and [OpenAI gym](https://gym.openai.com/). I created the [UnityMlFacade](unityml/unity_ml_facade.py) class. In doing this I opened up the possibilities of leveraging a number of frameworks and options to solve this problem. Since I was using [stable baselines 3](https://stable-baselines3.readthedocs.io/en/master/) with an OpenAI gym environment for a project I am involved in at work, I decided to incorporate this in my solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment architecture\n",
    "I have implemented an experiment based framework that allows for exploration of different hyperparameters when training a model. The parameters that you can specify are these:\n",
    "\n",
    "- **learning-rate** the learning rate to use for training the model. Default is 0.0003.\n",
    "- **batch-size** the size of batches that are sampled and used to train the model. Default is 100.\n",
    "- **buffer-size** the size of the replay buffer. Default is 100,000.\n",
    "- **total-timesteps** the total number of timesteps to train for. Default is 100,000.\n",
    "- **seed** random seed to use. Default is -1 which means generate a random value.\n",
    "- **environment-port** this is the port number used for communication with the Unity environment. If you want to have more than one agent running at the same time you would specify a different port for each of them. Default is 5005.\n",
    "- **policy-layers** the hidden layers to use in the neural network. Specified as a comma separated list. Default is \"400,300\".\n",
    "- **algorithm** the name of the algorithm to use to train the agent. At the moment only td3 is supported and this is the default value.\n",
    "- **executable-path** the path to the executable to run. Please see the notes about [setup project](Setup.md) to help with this.\n",
    "- **experiments-root** the root folder that experiment output will be written to.\n",
    "- **experiment-name** the name of the experiment. A subfolder will be created to the 'experiments-root' folder with this name and all output from this experiment will be written to it.\n",
    "- **reward-threshold** the reward value that the solution must attain over 100 consecutive episodes. Default is 30.0.\n",
    "- **gamma** the gamma value used for greedy strategy. Default is 0.99."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n",
    "I used the Single Reacher environent and trained a TD3 network using [stable baselines 3](https://stable-baselines3.readthedocs.io/en/master/). For a more detailed explanation of **TD3** please see my [TD3 Explanation notebook](TD3_Explanation.ipynb) in this repositoy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Parameters\n",
    "I was able to solve this with the following parameters:\n",
    "\n",
    "- **learning-rate** 0.0005\n",
    "- **batch-size** 100\n",
    "- **buffer-size** 100,000\n",
    "- **seed** 947171 (this was a randomly generated value)\n",
    "- **policy-layers** \"128,128,128\"\n",
    "\n",
    "As you can see from the graph below I achieved the target of 100 consecutive episodes with an average reward >= 30.0 after 188 episodes:\n",
    "\n",
    "![TD3 Results](td3_summary_scores.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution Model Weights\n",
    "I have included the solution model weights in this repository. They are in this file: [td3_checkpoint.pth](td3_checkpoint.pth)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ideas for future work\n",
    "\n",
    "1. At the moment only TD3 has been implemented. Stable-baselines3 supports other algorithms for single agent environments such as SAC, so these could easily be added for further exploration.\n",
    "\n",
    "2. Add the ability to support multiple agent environments. This would then enable other algorithms such as PPO to be used for further exploration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "* I consulted the book [Grokking Deep Reinforcement Learning](https://www.manning.com/books/grokking-deep-reinforcement-learning) for algorithms, maths and explanations.\n",
    "\n",
    "* I also consulted the **TD3** paper [Addressing Function Approximation Error in Actor-Critic Methods](https://arxiv.org/abs/1802.09477) authored by Scott Fujimoto (et al)., for maths and explanations."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
